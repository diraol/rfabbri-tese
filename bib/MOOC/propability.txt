*) eventos, variáveis randomicas
*) variaveis discretas, contínuas
*) Indempendencia, condicionamento
*) Distribuições uniforme, geométrica/exponencial (memoryless), 
*) PMF, PDF, joint, conditional, marginal.
*) IDEIA: notacao condicional para distribuições não condicionais. Assim, unifica-se as pmfs, pdfs, cdfs em sempre condicionadas. No caso "não-condicionado" a condição é que o evento pertença ao conjunto amostral. (óbvio, mas teoricamente vale).
*) Variáveis mistas
*) Bivariate distributions (gaussians with dependencies?)
*) Bayes rule: the foundation of inference. One discrete and one continuous RV ("4 variations").
*) Bayes, duas interpretações associando o denominador a cada um dos termos do numerador: a chance de ter ocorrido um fato dado uma observação é a chance de ocorrência do fato vezes a razão entre as chances da observação ocorrer dado o fato e sem condição alguma; é a razão entre as chances do fato e da observação vezes a chance da observação dada o fato.
*) Composite RVs, correlation and covarioation, conditional expectation.
*) THE PDF OF A GENERAL FUNCTION, Lec. 11, parte 2 do video tem o mapeamento do linear aa lei de potencia com gamma=2.
*) Deriva df/dt=d(1/t)/dt=1/t^2. Simetrico dt/df=1/f^2, ou seja, a densidade de periodo cai com o quadrado da frequencia, e a densidade de frequencia cai com o quadrado do periodo.
*) Convolution, Lec 12
*) Dependence, covariance.
*) Independencia => Covariance=0
*) Dependencia pode ambos covariance => 0 ou qualquer.
*) Covariance>0 => directly proportional
*) Covariance<0 => inversely proportional
*) Covariancia é ressonância!
Var(X+Y)=Var(X)+Var(Y)+Cov(X,Y)
*) Correlação E [-1.1] com -1,1 => determinismo linear.
*) Correlação acima de 1/2: mais da metade da variável aleatória tem fator (ou variável aleatória) em comum.
*) Investimentos não correlacionados são mais seguros! (o desvio padrão da soma dos investimentos é menor)
*) Law of iterated expectations
*) Law of total variance
*) Com estas leis de esperaça a variancia iteradas, parece que posso modelar as redes de interação, seus agentes e relacionar as médias gerais às médias dos setores, e dar parâmetros para o quanto a média representa somente os hubs ou o quanto dos periféricos ou intermediários precisam ser amostrados para obter uma mostragem da população, não da atividade.
*) Total variance vem da variancia entre os cenarios e da variancia em cada cenário: var(X) = E[var(X|Y)]+var(E[X|Y])
*) Geração de amostras de distribuições arbitrárias a partir da cumulativa e de 1 gerador de variavel uniforme.
*) A convolução de gaussianas independentes dá gaussiana. E a convolução da step? Como fica a sync?
*) Variance === Incerteza
*) Hyposthesis testing problems, estimations problems
*) Point estimates, LMS, MAP, performance metrics, 
*) "Hypothesis testings" => the appropriate metric is the probability of error, of making a mistake. For "estimating a numerical quantity", an appropriate metric is the square error.
*) Inference and statistics
*) Model building vs inferring unobserved variables
*) Relação: "mundo real", "inferência/estatística", "teoria de probabilidade/análise"
*) MAP estimates são qualificados e otimizam o "conditional prob of error" e o "overall prob of error".
*) Beta distributions and recursive updates of the posterior of Theta
*) Linear normal models. Linear regression. 
*) Em probabilidade, há o erro, há a marginal, peak, joint, cdf, fdp, condicionais. Isso se relacionada à diversidade, cobertura e fronteira de mundo, como transparecido pelas MAP (mapeamentto, contato, recebimento de sinal de existencia sobre) e distribution (análogo de coleta e difusão). Um psiquiatra facilmente observaria a área sobre esta ótica. A probabilidade é a teoria da chance. Não do fixo correto, mas da eterna mudançao, que serve à expansão, informatividade. Este é o papel dos periféricos, aponta para este lado da polaridade. "When we know the join, we can find the marginal".
*) Bayesian confidence intervals.
*) Least Mean Squares
*) LLMS
*) Linear in what? (a_1 * X + a_2 * X^2 +a_3 log (X) ... )
*) Classival or non-bayesian statistics.
*) desigualdades de markov e chebichev
*) Weak law of large numbers (WLLN)
*) Dualidade frequencia/probabilidade: se f=1/T, qual a relação da probabilidade com o tempo? Prob= 1 / (1+media de eventos entre cada ocorrencia).
*) Pooling Lec. 18 v5, relação entre exatidão, confiança e amostragem via desigualdade de chebichev.
*) convergência "na probabilidade"
*) associação da conv i.p. com o bulbo da distr, enquanto a média é sensível às caldas.
*) Chernoff bound
*) Central Limit Theorem
*) Transforms?
*) Approximations of the binomial by the normal. Moivre-Laplace approximation***.
*) Toda aresta é uma variável de bernouli. Toda rede é uma binomial.
*) Tamanho da amostragem, margem de erro e de confiança através de comparação com a normal. SEGUNDO O VIDEO, AS MARGENS FINAIS SAO COMPATIVEIS COM AS USADAS PELOS JORNAIS. LEC. 19.
Neste caso, pensaria em advocar o dialoga assim como as pesquisas datafolha e outras pois não temos como saber o quão bem coletadas foram. 
*) Bayesian vs Classical statistics (random variable vs unknown constant).
*) Freedom and art in classical statistics.
*) Estimator and the standard error.
*) Confidence intervals.
*)Using normal for finding
confidence intervals on sampling bernouli variables.
*)  t-tables. 
*) unbiased estimator of the variance? (X_i-OMEGA_n)/(n-1)
*) Maximum likelyhood estimator
*) Jansen's inequality
*) Hoeffding's inequality



